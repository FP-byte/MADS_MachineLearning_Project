{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets.base import BaseDatastreamer\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch import nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from src import datasets, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "\n",
    "datadir = Path('../data')\n",
    "configfile = Path(\"config.toml\")\n",
    "\n",
    "with configfile.open('rb') as f:\n",
    "    config = tomllib.load(f)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths\n",
    "trainfile = datadir / (config['arrhythmia'] + '_train.parq')\n",
    "testfile = datadir / (config['arrhythmia'] + '_test.parq')\n",
    "trainfile, testfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(trainfile)\n",
    "test_df = pd.read_parquet(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_df = train_df[train_df['target'] == 0]\n",
    "anom_train_df = train_df[train_df['target'] != 0]\n",
    "norm_test_df = test_df[test_df['target'] == 0]\n",
    "anom_test_df = test_df[test_df['target'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two separate datasets for normal and anomalous data\n",
    "\n",
    "# norm_test_df.to_parquet(datadir / 'heart_big_norm_train.parq')\n",
    "# anom_test_df.to_parquet(datadir / 'heart_big_anom_train.parq')\n",
    "\n",
    "# norm_test_df.to_parquet(datadir / 'heart_big_norm_test.parq')\n",
    "# anom_test_df.to_parquet(datadir / 'heart_big_anom_test.parq')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = datadir / (config['arrhythmia'] + '_norm_train.parq')\n",
    "testfile = datadir / (config['arrhythmia'] + '_norm_test.parq')\n",
    "trainfile, testfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the datasets \n",
    "traindataset = datasets.HeartDataset1D(trainfile, target=\"target\")\n",
    "testdataset = datasets.HeartDataset1D(testfile, target=\"target\")\n",
    "traindataset, testdataset\n",
    "\n",
    "# moving to mps device crashes the jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the datastreamers\n",
    "trainstreamer = BaseDatastreamer(traindataset, preprocessor = BasePreprocessor(), batchsize=32)\n",
    "teststreamer = BaseDatastreamer(testdataset, preprocessor = BasePreprocessor(), batchsize=32)\n",
    "len(trainstreamer), len(teststreamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape = (16, 12)\n",
    "# traindataset = datasets.HeartDataset2D(trainfile, target=\"target\", shape=shape)\n",
    "# testdataset = datasets.HeartDataset2D(testfile, target=\"target\", shape=shape)\n",
    "# traindataset, testdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainstreamer = BaseDatastreamer(traindataset, preprocessor = BasePreprocessor(), batchsize=32)\n",
    "# teststreamer = BaseDatastreamer(testdataset, preprocessor = BasePreprocessor(), batchsize=32)\n",
    "# len(trainstreamer), len(teststreamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(trainstreamer.stream())\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1micro = metrics.F1Score(average='micro')\n",
    "f1macro = metrics.F1Score(average='macro')\n",
    "precision = metrics.Precision('micro')\n",
    "recall = metrics.Recall('macro')\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mads_exam.db\")\n",
    "mlflow.set_experiment(\"Autoencoder model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from pydantic import BaseModel\n",
    "from mads_datasets.base import BaseDatastreamer\n",
    "class VAEstreamer(BaseDatastreamer):\n",
    "    def stream(self) -> Iterator:\n",
    "        while True:\n",
    "            if self.index > (self.size - self.batchsize):\n",
    "                self.reset_index()\n",
    "            batch = self.batchloop()\n",
    "            # we throw away the Y\n",
    "            X_, _ = zip(*batch)  # noqa N806\n",
    "            X = torch.stack(X_)  # noqa N806\n",
    "            # change the channel to channel-last\n",
    "            X = torch.moveaxis(X, 1, 2)  # noqa N806\n",
    "            # and yield X, X\n",
    "            yield X, X\n",
    "\n",
    "class VAESettings(BaseModel):\n",
    "    data_dir: Path = Path(\"data\")\n",
    "    h1: int = 192\n",
    "    h2: int = 100\n",
    "    insize: int = 2\n",
    "    latent: int = 2\n",
    "    batchsize: int = 32\n",
    "    epochs: int = 10\n",
    "    modelname: Path = Path(\"vaemodel.pt\")\n",
    "    modeldir: Path = Path(\"models\")\n",
    "    imgpath: Path = Path(\"img\")\n",
    "    samplesize: int = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import ReportTypes, Trainer, TrainerSettings, vae\n",
    "presets = VAESettings()\n",
    "print(presets)\n",
    "trainstreamer = VAEstreamer(traindataset, batchsize=presets.batchsize).stream()\n",
    "teststreamer = VAEstreamer(testdataset, batchsize=32).stream()\n",
    "#print(next(trainstreamer))\n",
    "X1, X2 = next(trainstreamer)\n",
    "X1.shape, X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = presets.model_dump()\n",
    "encoder = vae.Encoder(config)\n",
    "print(encoder)\n",
    "decoder = vae.Decoder(config)\n",
    "print(decoder)\n",
    "latent = encoder(x)\n",
    "latent = latent.view(32, 1, 16, 12)\n",
    "print(latent.shape)\n",
    "x = decoder(latent)\n",
    "lossfn = ReconstructionLoss()\n",
    "loss = lossfn(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReconstructionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReconstructionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        # Ensure yhat and y have the same shape\n",
    "        assert yhat.shape == y.shape, f\"Shape mismatch: yhat {yhat.shape}, y {y.shape}\"\n",
    "\n",
    "        # Compute squared error\n",
    "        sqe = (y - yhat) ** 2\n",
    "        \n",
    "        # Sum over batch and spatial dimensions (height, width, channels)\n",
    "        summed = sqe.sum(dim=(1, 2))  # Summing over batch, channel, height, and width\n",
    "        return summed.mean()  # Return the mean squared error over the batch\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)  # Flatten (32, 1, 16, 12) -> (32, 192)\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(192, 100),  # Flattened input size is 192\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 2)  # Output latent vector of size 2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Flatten input to [batch_size, 192]\n",
    "        x = self.encode(x)   # [batch_size, 2] (latent vector)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(2, 100),  # Latent vector size is 2 -> expand to 100\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 192),  # Expand to 192\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(192, 192)  # Expand to 192 for reshaping\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decode(x)  # [batch_size, 192]\n",
    "        x = x.view(-1, 1, 192)  # Reshape to [batch_size, 1, 16, 12]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "\n",
    "# Pass through the encoder\n",
    "latent = encoder(X1)  # Latent vector of shape [32, 2]\n",
    "\n",
    "# Pass through the decoder\n",
    "reconstructed = decoder(latent)  # Reconstructed shape should be [32, 1, 16, 12]\n",
    "print(reconstructed.shape)  # Should print torch.Size([32, 1, 16, 12])\n",
    "lossfn = ReconstructionLoss()\n",
    "print(X1.shape)\n",
    "loss = lossfn(X1, reconstructed)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"encoder\"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(config[\"insize\"], config[\"h1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[\"h1\"], config[\"h2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[\"h2\"], config[\"latent\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        latent = self.encode(x)\n",
    "        return latent\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config: Dict) -> None:\n",
    "        super().__init__()\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(config[\"latent\"], config[\"h2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[\"h2\"], config[\"h1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[\"h1\"], config[\"insize\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.decode(x)\n",
    "        x = x.reshape((-1, 28, 28, 1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, config: Dict) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        latent = self.encoder(x)\n",
    "        x = self.decoder(latent)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src import models\n",
    "#config Autoencoder\n",
    "config = {\n",
    "    \"insize\": 192,\n",
    "    \"h1\": 100,  \n",
    "    \"h2\": 40,\n",
    "    \"latent\": 2,\n",
    "    \n",
    "}\n",
    "autoencoder = AutoEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import Trainer, TrainerSettings, ReportTypes\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    optimizer = torch.optim.Adam\n",
    "\n",
    "    settings = TrainerSettings(\n",
    "        epochs=2,\n",
    "        metrics=[accuracy, f1micro, f1macro, precision, recall, loss_fn],\n",
    "        logdir=\"logs/heartAE\",\n",
    "        train_steps=len(list(trainstreamer))//5, #met 5 epochs heeft het een keer de hele dataset gezien\n",
    "        valid_steps=len(list(teststreamer))//5,\n",
    "        reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "        scheduler_kwargs=None,\n",
    "        earlystop_kwargs=None\n",
    "    )\n",
    "\n",
    "    # modify the tags when you change them!\n",
    "    mlflow.set_tag(\"model\", \"Autoencoder\")\n",
    "    mlflow.set_tag(\"dataset\", \"heart_big\")\n",
    "    mlflow.log_param(\"scheduler\", \"None\")\n",
    "    mlflow.log_param(\"earlystop\", \"None\")\n",
    "\n",
    "    mlflow.log_params(config)\n",
    "    mlflow.log_param(\"epochs\", settings.epochs)\n",
    "    mlflow.log_param(\"optimizer\", str(optimizer))\n",
    "    mlflow.log_params(settings.optimizer_kwargs)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=autoencoder,\n",
    "        settings=settings,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        traindataloader=trainstreamer.stream(),\n",
    "        validdataloader=teststreamer.stream(),\n",
    "        scheduler=None,\n",
    "        )\n",
    "    trainer.loop()\n",
    "\n",
    "    #htop om de gpu te zien\n",
    "    # dimensie transformer reduceren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = modeldir / \"vaemodel.pt\"\n",
    "\n",
    "torch.save(autoencoder, modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, streamer):\n",
    "  predictions, losses = [], []\n",
    "  criterion = nn.L1Loss(reduction='sum').to(device)\n",
    "  with torch.no_grad():\n",
    "    model = model.eval()\n",
    "    for x, y in next(teststreamer.stream()):\n",
    "      seq_true = y.to(device)\n",
    "      seq_pred = model(seq_true)\n",
    "\n",
    "      loss = criterion(seq_pred, seq_true)\n",
    "\n",
    "      predictions.append(seq_pred.cpu().numpy().flatten())\n",
    "      losses.append(loss.item())\n",
    "  return predictions, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = presets.modeldir / presets.modelname\n",
    "model = torch.load(modelpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, losses = predict(model, trainstreamer)\n",
    "\n",
    "sns.distplot(losses, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src import models\n",
    "#config GRU\n",
    "config = {\n",
    "    \"input\": 1,\n",
    "    \"hidden\": 256,  # updated key\n",
    "    \"dropout\": 0.1,\n",
    "    \"output\": 5,\n",
    "    \"num_layers\": 2,\n",
    "}\n",
    "model = GRUmodel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model GRU:\n",
    "\n",
    "Hyperparameters:\n",
    "- 256 hidden units\n",
    "- 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import Trainer, TrainerSettings, ReportTypes\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    optimizer = torch.optim.Adam\n",
    "\n",
    "    settings = TrainerSettings(\n",
    "        epochs=5,\n",
    "        metrics=[accuracy, f1micro, f1macro, precision, recall],\n",
    "        logdir=\"logs/heart2D\",\n",
    "        train_steps=len(trainstreamer) // 5, #met 5 epochs heeft het een keer de hele dataset gezien\n",
    "        valid_steps=len(teststreamer) // 5,\n",
    "        reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "        scheduler_kwargs=None,\n",
    "        earlystop_kwargs=None\n",
    "    )\n",
    "\n",
    "    # modify the tags when you change them!\n",
    "    mlflow.set_tag(\"model\", \"GRU\")\n",
    "    mlflow.set_tag(\"dataset\", \"heart_big\")\n",
    "    mlflow.log_param(\"scheduler\", \"None\")\n",
    "    mlflow.log_param(\"earlystop\", \"None\")\n",
    "\n",
    "    mlflow.log_params(config)\n",
    "    mlflow.log_param(\"epochs\", settings.epochs)\n",
    "    mlflow.log_param(\"optimizer\", str(optimizer))\n",
    "    mlflow.log_params(settings.optimizer_kwargs)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        settings=settings,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        traindataloader=trainstreamer.stream(),\n",
    "        validdataloader=teststreamer.stream(),\n",
    "        scheduler=None,\n",
    "        )\n",
    "    trainer.loop()\n",
    "\n",
    "    #htop om de gpu te zien\n",
    "    # dimensie transformer reduceren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "testdata = teststreamer.stream()\n",
    "for _ in range(len(teststreamer)):\n",
    "    X, y = next(testdata)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1) # we get the one with the highest probability\n",
    "    y_pred.append(yhat.cpu().tolist())\n",
    "    y_true.append(y.cpu().tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "\n",
    "plot = sns.heatmap(cfm, annot=cfm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "testdata = teststreamer.stream()\n",
    "for _ in range(len(teststreamer)):\n",
    "    X, y = next(testdata)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1) # we get the one with the highest probability\n",
    "    y_pred.append(yhat.cpu().tolist())\n",
    "    y_true.append(y.cpu().tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "\n",
    "plot = sns.heatmap(cfm, annot=cfm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "testdata = teststreamer.stream()\n",
    "for _ in range(len(teststreamer)):\n",
    "    X, y = next(testdata)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1) # we get the one with the highest probability\n",
    "    y_pred.append(yhat.cpu().tolist())\n",
    "    y_true.append(y.cpu().tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "\n",
    "plot = sns.heatmap(cfm, annot=cfm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "testdata = teststreamer.stream()\n",
    "for _ in range(len(teststreamer)):\n",
    "    X, y = next(testdata)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1) # we get the one with the highest probability\n",
    "    y_pred.append(yhat.cpu().tolist())\n",
    "    y_true.append(y.cpu().tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "\n",
    "plot = sns.heatmap(cfm, annot=cfm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
