{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.129'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mads_datasets.base import BaseDatastreamer\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch import nn\n",
    "import torch\n",
    "from mltrainer import Trainer, TrainerSettings, ReportTypes\n",
    "import tomllib\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Any, Union\n",
    "from src import datasets, metrics\n",
    "import mltrainer\n",
    "import mlflow\n",
    "from src import models\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "mltrainer.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrixs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, teststreamer):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    #model = model.to(\"cpu\")\n",
    "\n",
    "    #testdata = [(x.to(device), y.to(device)) for x, y in teststreamer.stream()]\n",
    "    testdata = teststreamer.stream()\n",
    "    for _ in range(len(teststreamer)):\n",
    "        X, y = next(testdata)\n",
    "\n",
    "        yhat = model(X)\n",
    "        yhat = yhat.argmax(dim=1) # we get the one with the highest probability\n",
    "        y_pred.append(yhat.cpu().tolist())\n",
    "        y_true.append(y.cpu().tolist())\n",
    "\n",
    "    yhat = [x for y in y_pred for x in y]\n",
    "    y = [x for y in y_true for x in y]\n",
    "    return y, yhat\n",
    "\n",
    "def plot_confusion_matrix(cfm, model_name):\n",
    "    # Create the plot using seaborn\n",
    "    plot = sns.heatmap(cfm, annot=True, fmt=\".3f\")\n",
    "    \n",
    "    # Set the labels for the axes\n",
    "    plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n",
    "    \n",
    "    # Save the plot to a file with the model name in the filename\n",
    "    plt.savefig(f\"{model_name}_confusion_matrix.png\", bbox_inches=\"tight\")\n",
    "    \n",
    "    # Optionally, display the plot\n",
    "    plt.show()\n",
    "\n",
    "# difference between test and train\n",
    "def calculate_difference(train_scores_str, test_scores_str):\n",
    "    # Remove the brackets and split the string by spaces\n",
    "    str_list = test_scores_str.strip('[]').split()\n",
    "\n",
    "    # Convert each element to a float\n",
    "    test_scores_str = [float(num) for num in str_list]\n",
    "\n",
    "    test_scores = [float(x) for x in test_scores_str]\n",
    "    train_scores = [float(x) for x in train_scores_str]\n",
    "    difference = [str(np.round(test - train, 3)) for test, train in zip(test_scores, train_scores)]\n",
    "    print(f'train loss average:{np.round(np.mean(test_scores), 3)}')\n",
    "    print(f'test loss average:{np.round(np.mean(test_scores), 3)}')\n",
    "\n",
    "    return difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainstreamer, teststreamer, config, settings=None):\n",
    "\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            optimizer = torch.optim.Adam\n",
    "\n",
    "            if settings==None:\n",
    "                # default settings\n",
    "                settings = TrainerSettings(\n",
    "                    epochs=15,\n",
    "                    metrics=[accuracy, f1micro, f1macro, precision, recall],\n",
    "                    logdir=\"logs/heart2D\",\n",
    "                    train_steps=len(trainstreamer) // 5,\n",
    "                    valid_steps=len(teststreamer) // 5,\n",
    "                    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "                    scheduler_kwargs={\"factor\": 0.5, \"patience\": 2},\n",
    "                    earlystop_kwargs={\"patience\": 8},\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                settings=settings,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                traindataloader=trainstreamer.stream(),\n",
    "                validdataloader=teststreamer.stream(),\n",
    "                scheduler= config[\"scheduler\"],\n",
    "            )\n",
    "\n",
    "            mlflow.set_tag(\"model\", config[\"model\"])\n",
    "            mlflow.set_tag(\"dataset\", \"heart_big_oversampled\")\n",
    "            mlflow.log_param(\"scheduler\", str(trainer.scheduler).split(\".\")[-1])\n",
    "            mlflow.log_param(\"earlystop\", str(settings.earlystop_kwargs))\n",
    "\n",
    "            base_config = {\n",
    "                \"hidden\": config[\"hidden\"],\n",
    "                \"dropout\": config[\"dropout\"],\n",
    "                \"num_classes\": config[\"num_classes\"],\n",
    "                \"num_heads\": config[\"num_heads\"],\n",
    "                \"num_blocks\": config[\"num_blocks\"],\n",
    "               # \"shape\": config[\"shape\"]\n",
    "            }\n",
    "            mlflow.log_params(base_config)\n",
    "            mlflow.log_param(\"epochs\", settings.epochs)\n",
    "            mlflow.log_param(\"shape0\", config[\"shape\"])\n",
    "            mlflow.log_param(\"optimizer\", str(optimizer))\n",
    "            mlflow.log_params(settings.optimizer_kwargs)\n",
    "            trainer.loop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ptb': 'heart', 'arrhythmia': 'heart_big'}\n",
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "datadir = Path('../data')\n",
    "configfile = Path(\"config.toml\")\n",
    "\n",
    "with configfile.open('rb') as f:\n",
    "    config = tomllib.load(f)\n",
    "print(config)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')\n",
    "            print('MPS is available')\n",
    "else:\n",
    "            device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('../data/heart_big_oversampled_full_train.parq'),\n",
       " PosixPath('../data/heart_big_test.parq'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choos dataset\n",
    "trainfile = datadir / (config['arrhythmia'] + '_oversampled_full_train.parq')\n",
    "#trainfile = datadir / (config['arrhythmia'] + '_SMOTE_train.parq')\n",
    "testfile = datadir / (config['arrhythmia'] + '_test.parq')\n",
    "trainfile, testfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D dataset\n",
    "traindataset = datasets.HeartDataset1D(trainfile, target=\"target\")\n",
    "testdataset = datasets.HeartDataset1D(testfile, target=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8593, 684)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainstreamer = BaseDatastreamer(traindataset, preprocessor = BasePreprocessor(), batchsize=32)\n",
    "teststreamer = BaseDatastreamer(testdataset, preprocessor = BasePreprocessor(), batchsize=32)\n",
    "len(trainstreamer), len(teststreamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 192, 1]), torch.Size([32]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer.stream())\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 1D + RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1micro = metrics.F1Score(average='micro')\n",
    "f1macro = metrics.F1Score(average='macro')\n",
    "precision = metrics.Precision('micro')\n",
    "recall = metrics.Recall('macro')\n",
    "accuracy = metrics.Accuracy()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "        epochs=10,\n",
    "        metrics=[accuracy, f1micro, f1macro, precision, recall],\n",
    "        logdir=\"logs/heart1D_oversampled\",\n",
    "        train_steps=len(trainstreamer) // 5, #met 5 epochs heeft het een keer de hele dataset gezien\n",
    "        valid_steps=len(teststreamer) // 5,\n",
    "        reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "        scheduler_kwargs={\"factor\": 0.5, \"patience\": 2},\n",
    "        earlystop_kwargs= {\"patience\": 8},\n",
    "        device= device\n",
    "    )\n",
    "\n",
    "modelname= \"CNN1DResNet\"\n",
    "config = {\n",
    "    \"model\": modelname,\n",
    "    \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    \"input\": 1,\n",
    "    \"hidden\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_classes\": 5,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_blocks\": 2,\n",
    "    \"num_classes\": 5,\n",
    "    \"input_length\": 192,\n",
    "    \"shape\":(1, 192),\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "print(config)\n",
    "model = models.CNN1DResNet(config)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mads_exam.db\")\n",
    "mlflow.set_experiment(config[\"model\"])\n",
    "train_model(trainstreamer, teststreamer, config, settings)\n",
    "y, yhat = test_model(model, teststreamer)\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "confusion_matrixs[modelname] = cfm\n",
    "print(config)\n",
    "plot_confusion_matrix(cfm, modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYBRID MODEL 1D CNN + GRU + RESNET\n",
    "To speed up the trining, combining a CNN with GRU seemed like a better option\n",
    "The cnn runs in parallel instead of sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname= \"CNN1DGRUResNet\"\n",
    "config = {\n",
    "    \"model\": modelname,\n",
    "    \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    \"gru_hidden\": 128,\n",
    "    \"hidden\": 128,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_layers\": 5,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_blocks\": 1,\n",
    "    \"num_classes\": 5,\n",
    "    \"input_length\": 192, \n",
    "    \"input_size\": 1,  \n",
    "}\n",
    "print(config)\n",
    "model = models.CNN1DGRUResNet(config)\n",
    "print(model)\n",
    "summary(model, input_size=(192, 1))\n",
    "mlflow.set_tracking_uri(\"sqlite:///mads_exam.db\")\n",
    "mlflow.set_experiment(config[\"model\"])\n",
    "train_model(trainstreamer, teststreamer, config, settings)\n",
    "y, yhat = test_model(model, teststreamer)\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "confusion_matrixs[modelname] = cfm\n",
    "print(config)\n",
    "plot_confusion_matrix(cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D TRANSFORMER MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## TRANSFORMER 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1micro = metrics.F1Score(average='micro')\n",
    "f1macro = metrics.F1Score(average='macro')\n",
    "precision = metrics.Precision('micro')\n",
    "recall = metrics.Recall('macro')\n",
    "accuracy = metrics.Accuracy()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "        epochs=15,\n",
    "        metrics=[accuracy, f1micro, f1macro, precision, recall],\n",
    "        logdir=\"logs/heart1D_oversampled\",\n",
    "        train_steps=len(trainstreamer) // 5, #met 5 epochs heeft het een keer de hele dataset gezien\n",
    "        valid_steps=len(teststreamer) // 5,\n",
    "        reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "        scheduler_kwargs={\"factor\": 0.5, \"patience\": 2},\n",
    "        earlystop_kwargs= {\"patience\": 8},\n",
    "        device= device\n",
    "    )\n",
    "\n",
    "modelname= \"Transformer1D\"\n",
    "config = {\n",
    "    \"model\": modelname,\n",
    "    \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    \"input\": 1,\n",
    "    \"hidden\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_classes\": 5,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_blocks\": 2,\n",
    "    \"num_classes\": 5,\n",
    "    \"shape\": (16, 12),\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "print(config)\n",
    "model = models.Transformer(config)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mads_exam.db\")\n",
    "mlflow.set_experiment(config[\"model\"])\n",
    "train_model(trainstreamer, teststreamer, config, settings)\n",
    "y, yhat = test_model(model, teststreamer)\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "confusion_matrixs[modelname] = cfm\n",
    "print(config)\n",
    "plot_confusion_matrix(cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D transformer + RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1micro = metrics.F1Score(average='micro')\n",
    "f1macro = metrics.F1Score(average='macro')\n",
    "precision = metrics.Precision('micro')\n",
    "recall = metrics.Recall('macro')\n",
    "accuracy = metrics.Accuracy()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "        epochs=15,\n",
    "        metrics=[accuracy, f1micro, f1macro, precision, recall],\n",
    "        logdir=\"logs/heart1D_oversampled\",\n",
    "        train_steps=len(trainstreamer) // 5, #met 5 epochs heeft het een keer de hele dataset gezien\n",
    "        valid_steps=len(teststreamer) // 5,\n",
    "        reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "        scheduler_kwargs={\"factor\": 0.5, \"patience\": 2},\n",
    "        earlystop_kwargs= {\"patience\": 8},\n",
    "        device= device\n",
    "    )\n",
    "\n",
    "modelname= \"Transformer1DResnet\"\n",
    "config = {\n",
    "    \"model\": modelname,\n",
    "    \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    \"input\": 1,\n",
    "    \"hidden\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_classes\": 5,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_blocks\": 2,\n",
    "    \"num_classes\": 5,\n",
    "    \"shape\": (16, 12),\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "print(config)\n",
    "model = models.Transformer1DResnet(config)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mads_exam.db\")\n",
    "mlflow.set_experiment(config[\"model\"])\n",
    "train_model(trainstreamer, teststreamer, config, settings)\n",
    "y, yhat = test_model(model, teststreamer)\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "confusion_matrixs[modelname] = cfm\n",
    "print(config)\n",
    "plot_confusion_matrix(cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D TRANSFOMER + RESNET + SQUEEZE AND EXCITE\n",
    "- The SE block would help compact the feature representation through global pooling and speed traing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1micro = metrics.F1Score(average='micro')\n",
    "f1macro = metrics.F1Score(average='macro')\n",
    "precision = metrics.Precision('micro')\n",
    "recall = metrics.Recall('macro')\n",
    "accuracy = metrics.Accuracy()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "        epochs=15,\n",
    "        metrics=[accuracy, f1micro, f1macro, precision, recall],\n",
    "        logdir=\"logs/heart1D_oversampled\",\n",
    "        train_steps=len(trainstreamer) // 5, #met 5 epochs heeft het een keer de hele dataset gezien\n",
    "        valid_steps=len(teststreamer) // 5,\n",
    "        reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "        scheduler_kwargs={\"factor\": 0.5, \"patience\": 2},\n",
    "        earlystop_kwargs= {\"patience\": 8},\n",
    "        device= device\n",
    "    )\n",
    "\n",
    "modelname= \"Transformer1DResnetSE\"\n",
    "config = {\n",
    "    \"model\": modelname,\n",
    "    \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    \"input\": 1,\n",
    "    \"hidden\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_classes\": 5,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_blocks\": 2,\n",
    "    \"num_classes\": 5,\n",
    "    \"shape\": (16, 12),\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "print(config)\n",
    "model = models.Transformer1DResnetSE(config)\n",
    "yhat = model(x)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mads_exam.db\")\n",
    "mlflow.set_experiment(config[\"model\"])\n",
    "train_model(trainstreamer, teststreamer, config, settings)\n",
    "y, yhat = test_model(model, teststreamer)\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "confusion_matrixs[modelname] = cfm\n",
    "print(config)\n",
    "plot_confusion_matrix(cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMER + RESNET + SE + MULTIHEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1micro = metrics.F1Score(average='micro')\n",
    "f1macro = metrics.F1Score(average='macro')\n",
    "precision = metrics.Precision('micro')\n",
    "recall = metrics.Recall('macro')\n",
    "accuracy = metrics.Accuracy()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "        epochs=15,\n",
    "        metrics=[accuracy, f1micro, f1macro, precision, recall],\n",
    "        logdir=\"logs/heart1D_oversampled\",\n",
    "        train_steps=len(trainstreamer) // 5, #met 5 epochs heeft het een keer de hele dataset gezien\n",
    "        valid_steps=len(teststreamer) // 5,\n",
    "        reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "        scheduler_kwargs={\"factor\": 0.5, \"patience\": 2},\n",
    "        earlystop_kwargs= {\"patience\": 8},\n",
    "        device= device\n",
    "    )\n",
    "\n",
    "modelname= \"Transformer1DResnetSEwithAttention\"\n",
    "config = {\n",
    "    \"model\": modelname,\n",
    "    \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    \"input\": 1,\n",
    "    \"hidden\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_classes\": 5,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_blocks\": 2,\n",
    "    \"num_classes\": 5,\n",
    "    \"shape\": (16, 12),\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "print(config)\n",
    "model = models.Transformer1DResnetSEwithAttention(config)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mads_exam.db\")\n",
    "mlflow.set_experiment(config[\"model\"])\n",
    "train_model(trainstreamer, teststreamer, config, settings)\n",
    "y, yhat = test_model(model, teststreamer)\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "confusion_matrixs[modelname] = cfm\n",
    "print(config)\n",
    "plot_confusion_matrix(cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # testing types of seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed from Date and Time (hashed): 1902171767351478533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2025-01-30_11-57-17'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get current date and time in a specific format (e.g., '2025-01-28_15-30-45')\n",
    "current_time_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Hash the string to create a seed\n",
    "random_seed = hash(current_time_str)\n",
    "print(f\"Random Seed from Date and Time (hashed): {random_seed}\")\n",
    "current_time_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed from Date and Time: 1738234650\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Use the current date and time to generate a more unique seed\n",
    "current_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "random_seed = int(time.mktime(time.strptime(current_time_str, \"%Y-%m-%d %H:%M:%S\")))\n",
    "print(f\"Random Seed from Date and Time: {random_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed: 2921403402\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Generate a random integer between 0 and 2^32 - 1\n",
    "random_seed = random.randint(0, 2**32 - 1)\n",
    "print(f\"Random Seed: {random_seed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
